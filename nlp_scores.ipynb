{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EIjyTGR3SWCJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "of4MH16J0z-p",
        "outputId": "09835933-08c7-4b59-91b5-69bbc38bc456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master dictionary has 86553 words.\n",
            "Processed Row_No: 0\n",
            "Processed Row_No: 1\n",
            "Processed Row_No: 2\n",
            "Processed Row_No: 3\n",
            "Processed Row_No: 4\n",
            "Processed Row_No: 5\n",
            "Processed Row_No: 6\n",
            "Processed Row_No: 7\n",
            "Processed Row_No: 8\n",
            "Processed Row_No: 9\n",
            "Processed Row_No: 10\n",
            "Processed Row_No: 11\n",
            "Processed Row_No: 12\n",
            "Processed Row_No: 13\n",
            "Processed Row_No: 14\n",
            "Processed Row_No: 15\n",
            "Processed Row_No: 16\n",
            "Processed Row_No: 17\n",
            "Processed Row_No: 18\n",
            "Processed Row_No: 19\n",
            "Processed Row_No: 20\n",
            "Processed Row_No: 21\n",
            "Processed Row_No: 22\n",
            "Processed Row_No: 23\n",
            "Processed Row_No: 24\n",
            "Processed Row_No: 25\n",
            "Processed Row_No: 26\n",
            "Processed Row_No: 27\n",
            "Processed Row_No: 28\n",
            "Processed Row_No: 29\n",
            "Processed Row_No: 30\n",
            "Processed Row_No: 31\n",
            "Processed Row_No: 32\n",
            "Processed Row_No: 33\n",
            "Processed Row_No: 34\n",
            "Processed Row_No: 35\n",
            "Processed Row_No: 36\n",
            "Processed Row_No: 37\n",
            "Processed Row_No: 45\n",
            "Processed Row_No: 46\n",
            "Processed Row_No: 47\n",
            "Processed Row_No: 48\n",
            "Processed Row_No: 49\n",
            "Processed Row_No: 50\n",
            "Processed Row_No: 51\n",
            "Processed Row_No: 52\n",
            "Processed Row_No: 53\n",
            "Processed Row_No: 54\n",
            "Processed Row_No: 55\n",
            "Processed Row_No: 56\n",
            "Processed Row_No: 57\n",
            "Processed Row_No: 58\n",
            "Processed Row_No: 59\n",
            "Processed Row_No: 60\n",
            "Processed Row_No: 61\n",
            "Processed Row_No: 62\n",
            "Processed Row_No: 63\n",
            "Processed Row_No: 64\n",
            "Processed Row_No: 65\n",
            "Processed Row_No: 66\n",
            "Processed Row_No: 67\n",
            "Processed Row_No: 68\n",
            "Processed Row_No: 69\n",
            "Processed Row_No: 70\n",
            "Processed Row_No: 71\n",
            "Processed Row_No: 72\n",
            "Processed Row_No: 73\n",
            "Processed Row_No: 74\n",
            "Processed Row_No: 75\n",
            "Processed Row_No: 76\n",
            "Processed Row_No: 77\n",
            "Processed Row_No: 78\n",
            "Processed Row_No: 79\n",
            "Processed Row_No: 80\n",
            "Processed Row_No: 81\n",
            "Processed Row_No: 82\n",
            "Processed Row_No: 83\n",
            "Processed Row_No: 84\n",
            "Processed Row_No: 85\n",
            "Processed Row_No: 86\n",
            "Processed Row_No: 87\n",
            "Processed Row_No: 88\n",
            "Processed Row_No: 89\n",
            "Processed Row_No: 90\n",
            "Processed Row_No: 91\n",
            "Processed Row_No: 92\n",
            "Processed Row_No: 93\n",
            "Processed Row_No: 94\n",
            "Processed Row_No: 97\n",
            "Processed Row_No: 98\n",
            "Processed Row_No: 99\n",
            "Processed Row_No: 100\n",
            "Processed Row_No: 101\n",
            "Processed Row_No: 105\n",
            "Processed Row_No: 106\n",
            "Processed Row_No: 107\n",
            "Processed Row_No: 108\n",
            "Processed Row_No: 109\n",
            "Processed Row_No: 110\n",
            "Processed Row_No: 111\n",
            "Processed Row_No: 112\n",
            "Processed Row_No: 113\n",
            "Processed Row_No: 114\n",
            "Processed Row_No: 115\n",
            "Processed Row_No: 116\n",
            "Processed Row_No: 117\n",
            "Processed Row_No: 118\n",
            "Processed Row_No: 119\n",
            "Processed Row_No: 120\n",
            "Processed Row_No: 121\n",
            "Processed Row_No: 123\n",
            "Processed Row_No: 124\n",
            "Processed Row_No: 125\n",
            "Processed Row_No: 126\n",
            "Processed Row_No: 127\n",
            "Processed Row_No: 128\n",
            "Processed Row_No: 129\n",
            "Processed Row_No: 130\n",
            "Processed Row_No: 131\n",
            "Processed Row_No: 132\n",
            "Processed Row_No: 133\n",
            "Processed Row_No: 134\n",
            "Processed Row_No: 135\n",
            "Processed Row_No: 136\n",
            "Processed Row_No: 137\n",
            "Processed Row_No: 139\n",
            "Processed Row_No: 140\n",
            "Processed Row_No: 141\n",
            "Processed Row_No: 142\n",
            "Processed Row_No: 143\n",
            "Processed Row_No: 144\n",
            "Processed Row_No: 145\n",
            "Processed Row_No: 146\n",
            "Processed Row_No: 147\n",
            "Processed Row_No: 148\n",
            "Processed Row_No: 149\n",
            "Processed Row_No: 150\n",
            "Processed Row_No: 151\n",
            "Processed Row_No: 152\n",
            "Processed Row_No: 153\n",
            "Processed Row_No: 154\n",
            "Processed Row_No: 167\n",
            "Processed Row_No: 168\n",
            "Processed Row_No: 169\n",
            "Processed Row_No: 170\n",
            "Processed Row_No: 171\n",
            "Processed Row_No: 172\n",
            "Processed Row_No: 173\n",
            "Processed Row_No: 174\n",
            "Processed Row_No: 175\n",
            "Processed Row_No: 176\n",
            "Processed Row_No: 179\n",
            "Processed Row_No: 180\n",
            "Processed Row_No: 181\n",
            "Processed Row_No: 182\n",
            "Processed Row_No: 183\n",
            "Processed Row_No: 189\n",
            "Processed Row_No: 190\n",
            "Processed Row_No: 191\n",
            "Processed Row_No: 192\n",
            "Processed Row_No: 193\n",
            "Processed Row_No: 194\n",
            "Processed Row_No: 195\n",
            "Processed Row_No: 196\n",
            "Processed Row_No: 197\n",
            "Processed Row_No: 198\n",
            "Processed Row_No: 199\n",
            "Processed Row_No: 200\n",
            "Processed Row_No: 201\n",
            "Processed Row_No: 202\n",
            "Processed Row_No: 203\n",
            "Processed Row_No: 204\n",
            "Processed Row_No: 205\n",
            "Processed Row_No: 211\n",
            "Processed Row_No: 212\n",
            "Processed Row_No: 213\n",
            "Processed Row_No: 214\n",
            "Processed Row_No: 215\n",
            "Processed Row_No: 216\n",
            "Processed Row_No: 217\n",
            "Processed Row_No: 219\n",
            "Processed Row_No: 220\n",
            "Processed Row_No: 221\n",
            "Processed Row_No: 222\n",
            "Processed Row_No: 223\n",
            "Processed Row_No: 224\n",
            "Processed Row_No: 225\n",
            "Processed Row_No: 226\n",
            "Processed Row_No: 227\n",
            "Processed Row_No: 228\n",
            "Processed Row_No: 229\n",
            "Processed Row_No: 234\n",
            "Processed Row_No: 235\n",
            "Processed Row_No: 236\n",
            "Processed Row_No: 237\n",
            "Processed Row_No: 238\n",
            "Processed Row_No: 239\n",
            "Processed Row_No: 240\n",
            "Processed Row_No: 241\n",
            "Processed Row_No: 242\n",
            "Processed Row_No: 243\n",
            "Processed Row_No: 244\n",
            "Processed Row_No: 245\n",
            "Processed Row_No: 246\n",
            "Processed Row_No: 247\n",
            "Processed Row_No: 248\n",
            "Processed Row_No: 249\n",
            "Processed Row_No: 250\n",
            "Processed Row_No: 251\n",
            "Processed Row_No: 252\n",
            "Processed Row_No: 253\n",
            "Processed Row_No: 254\n",
            "Processed Row_No: 255\n",
            "Processed Row_No: 256\n",
            "Processed Row_No: 257\n",
            "Processed Row_No: 258\n",
            "Processed Row_No: 259\n",
            "Processed Row_No: 260\n",
            "Processed Row_No: 261\n",
            "Processed Row_No: 262\n",
            "Processed Row_No: 263\n",
            "Processed Row_No: 264\n",
            "Processed Row_No: 265\n",
            "Processed Row_No: 266\n",
            "Processed Row_No: 273\n",
            "Processed Row_No: 274\n",
            "Processed Row_No: 275\n",
            "Processed Row_No: 276\n",
            "Processed Row_No: 277\n",
            "Processed Row_No: 278\n",
            "Processed Row_No: 279\n",
            "Processed Row_No: 280\n",
            "Processed Row_No: 281\n",
            "Processed Row_No: 282\n",
            "Processed Row_No: 283\n",
            "Processed Row_No: 284\n",
            "Processed Row_No: 285\n",
            "Processed Row_No: 286\n",
            "Processed Row_No: 288\n",
            "Processed Row_No: 289\n",
            "Processed Row_No: 290\n",
            "Processed Row_No: 291\n",
            "Processed Row_No: 292\n",
            "Processed Row_No: 293\n",
            "Processed Row_No: 294\n",
            "Processed Row_No: 295\n",
            "Processed Row_No: 296\n",
            "Processed Row_No: 297\n",
            "Processed Row_No: 298\n",
            "Processed Row_No: 299\n",
            "Processed Row_No: 304\n",
            "Processed Row_No: 305\n",
            "Processed Row_No: 306\n",
            "Processed Row_No: 307\n",
            "Processed Row_No: 308\n",
            "Processed Row_No: 309\n",
            "Processed Row_No: 310\n",
            "Processed Row_No: 311\n",
            "Processed Row_No: 312\n",
            "Processed Row_No: 313\n",
            "Processed Row_No: 314\n",
            "Processed Row_No: 315\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sentiment and word count analysis\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m master_dictionary\u001b[38;5;241m.\u001b[39mget(token):\n\u001b[0;32m     88\u001b[0m         total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     89\u001b[0m         word_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(token)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "\n",
        "def utf8len(s):\n",
        "    \"\"\"Helper function to get the size of a string.\"\"\"\n",
        "    return len(s.encode(\"utf-8\"))\n",
        "\n",
        "# Load your master dictionary file\n",
        "master_dictionary_file = \"Loughran-McDonald_MasterDictionary_1993-2021.csv\"\n",
        "co = 0\n",
        "# Sentiment output fields\n",
        "SENTIMENT_OUTPUT_FIELDS = {\n",
        "    \"Negative\": 1,\n",
        "    \"Positive\": 1,\n",
        "    \"Uncertainty\": 1,\n",
        "    \"Litigious\": 1,\n",
        "    \"Strong_Modal\": 1,\n",
        "    \"Weak_Modal\": 1,\n",
        "    \"Constraining\": 1\n",
        "}\n",
        "\n",
        "# Load the master dictionary into a Python dictionary\n",
        "master_dictionary = {}\n",
        "with open(master_dictionary_file) as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
        "    for row in csv_reader:\n",
        "        master_dictionary[row[\"Word\"]] = row\n",
        "print(f\"Master dictionary has {len(master_dictionary)} words.\")\n",
        "\n",
        "# Output fields\n",
        "FIXED_OUTPUT_FIELDS = [\n",
        "    \"ID\",  # 0\n",
        "    \"Name\",  # 1\n",
        "    \"Filing_Date\",  # 2\n",
        "]\n",
        "\n",
        "for key in SENTIMENT_OUTPUT_FIELDS.keys():\n",
        "    FIXED_OUTPUT_FIELDS.append(f\"{key}\")\n",
        "FIXED_OUTPUT_FIELDS.append( \"Readability\")\n",
        "\n",
        "\n",
        "data = [FIXED_OUTPUT_FIELDS]\n",
        "\n",
        "# CSV file containing MD&A data\n",
        "mda_data_file = \"compiled_dataset.csv\"  # Change this to your actual file path\n",
        "\n",
        "# Open the CSV containing MD&A text\n",
        "\n",
        "\n",
        "mdata = pd.read_csv(mda_data_file)\n",
        "mda_column = np.asarray(mdata[\"MDA Data\"])\n",
        "\n",
        "    # Iterate over each row of the input CSV\n",
        "\n",
        "for i in range (len(mdata)):\n",
        "    row = mdata.iloc[i]\n",
        "    cik = row['Name']\n",
        "    filing_date = row['Date']\n",
        "    mda_text = row['MDA Data'].upper()  # Convert the MD&A section to uppercase\n",
        "    if(len(mda_text)<50):\n",
        "        continue\n",
        "    # Customize tokenization here\n",
        "    tokens = re.findall(r\"\\w+\", mda_text)\n",
        "    co = co + 1\n",
        "    vocabulary = {}\n",
        "    output_data = [0] * len(FIXED_OUTPUT_FIELDS)\n",
        "    output_data[0] = co  # Accession_No\n",
        "    output_data[1] = cik  # CIK\n",
        "    output_data[2] = filing_date  # Filing_Date\n",
        "    #output_data[3] = utf8len(mda_text)  # Text_Size (Bytes)\n",
        "\n",
        "    #output_data[5] = len(re.findall(\"[A-Z]\", mda_text))  # Number_of_Alphabetic\n",
        "    #output_data[6] = len(re.findall(\"[0-9]\", mda_text))  # Number_of_Digits\n",
        "    number_doc = re.sub(r\"(?!=[0-9])(\\.|,)(?=[0-9])\", \"\", mda_text)\n",
        "    number_doc = number_doc.translate(\n",
        "        str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
        "    )\n",
        "    #output_data[7] = len(re.findall(r\"\\b[-+\\(]?[$€£]?[-+(]?\\d+\\)?\\b\", number_doc))  # Number_of_Numbers\n",
        "\n",
        "    total_syllables = 0\n",
        "    word_length = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Sentiment and word count analysis\n",
        "    for token in tokens:\n",
        "        if not token.isdigit() and len(token) > 1 and master_dictionary.get(token):\n",
        "            total_tokens += 1\n",
        "            word_length += len(token)\n",
        "\n",
        "            if token not in vocabulary:\n",
        "                vocabulary[token] = 1\n",
        "\n",
        "            total_syllables += int(master_dictionary[token][\"Syllables\"])\n",
        "\n",
        "            # Check sentiment for each token\n",
        "            for key, item in SENTIMENT_OUTPUT_FIELDS.items():\n",
        "                if master_dictionary[token][key] != \"0\":\n",
        "                    output_data[FIXED_OUTPUT_FIELDS.index(key)] += item\n",
        "\n",
        "    #output_data[4] = total_tokens  # Number_of_Words\n",
        "    #output_data[8] = total_syllables / total_tokens  # Average_Syllables\n",
        "    #output_data[9] = word_length / total_tokens  # Average_Word_Length\n",
        "    #output_data[10] = len(vocabulary)  # Vocabulary\n",
        "\n",
        "    # Convert values to percentage for sentiment columns\n",
        "    for j in range(3,10):\n",
        "        output_data[j] = (output_data[j] / total_tokens) * 100\n",
        "    output_data[10] = textstat.gunning_fog(mda_text)\n",
        "\n",
        "    print(f\"Processed Row_No: {i}\")\n",
        "    data.append(output_data)\n",
        "\n",
        "# Write the output to a CSV file\n",
        "output_file = \"sentiment_analysis_results.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as result_file:\n",
        "    writer = csv.writer(result_file)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Sentiment analysis complete. Results saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I5K4z3SZd6v",
        "outputId": "07c3fb2f-e1b9-448a-f438-9671260a677a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sentiment_analysis_results.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m compiled_df \u001b[38;5;241m=\u001b[39m compiled_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMDA Data\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 4: Read the sentiment analysis results (from the 4th column to the last)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sentiment_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_analysis_results.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Select columns from 4th till last using iloc (1-indexed position means 4th column corresponds to iloc[:, 3:])\u001b[39;00m\n\u001b[0;32m     16\u001b[0m sentiment_columns \u001b[38;5;241m=\u001b[39m sentiment_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m3\u001b[39m:]\n",
            "File \u001b[1;32mc:\\Users\\aaditya\\Desktop\\aaditya\\ml-dl\\ml_project_1\\Performance_Prediction\\proj_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aaditya\\Desktop\\aaditya\\ml-dl\\ml_project_1\\Performance_Prediction\\proj_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\aaditya\\Desktop\\aaditya\\ml-dl\\ml_project_1\\Performance_Prediction\\proj_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aaditya\\Desktop\\aaditya\\ml-dl\\ml_project_1\\Performance_Prediction\\proj_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\aaditya\\Desktop\\aaditya\\ml-dl\\ml_project_1\\Performance_Prediction\\proj_env\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentiment_analysis_results.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the original compiled dataset and remove the 'MDA Data' column\n",
        "compiled_df = pd.read_csv('compiled_dataset.csv')\n",
        "\n",
        "# Step 2: Remove rows where 'MDA Data' column length is less than 60\n",
        "compiled_df = compiled_df[compiled_df['MDA Data'].str.len() >= 60]\n",
        "\n",
        "# Step 3: Drop the 'MDA Data' column\n",
        "compiled_df = compiled_df.drop(columns=['MDA Data'])\n",
        "\n",
        "# Step 4: Read the sentiment analysis results (from the 4th column to the last)\n",
        "sentiment_df = pd.read_csv('sentiment_analysis_results.csv')\n",
        "\n",
        "# Select columns from 4th till last using iloc (1-indexed position means 4th column corresponds to iloc[:, 3:])\n",
        "sentiment_columns = sentiment_df.iloc[:, 3:]\n",
        "\n",
        "# Step 5: Concatenate the dataframes column-wise\n",
        "final_df = pd.concat([compiled_df, sentiment_columns], axis=1)\n",
        "\n",
        "# Step 6: Save the result to a new CSV file\n",
        "final_df.to_csv('final_dataset.csv', index=False)\n",
        "\n",
        "print(\"Final dataset created and saved as 'final_dataset.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFkiZXO2IYys"
      },
      "outputs": [],
      "source": [
        "# Open the CSV file in 'w+' mode\n",
        "with open(\"output_file.csv\", \"w\") as file:\n",
        "    write = csv.writer(file)\n",
        "    write.writerows(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz8N1nsHIt15"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
